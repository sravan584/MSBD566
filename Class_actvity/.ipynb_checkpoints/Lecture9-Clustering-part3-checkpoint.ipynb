{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac36aaf4",
   "metadata": {},
   "source": [
    "# MSBD 566 - Lecture 9\n",
    "## Clustering - Mean Shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a915619",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import modules\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e59a888",
   "metadata": {},
   "source": [
    "### Make data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce7de94",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs, make_classification, make_moons, make_circles\n",
    "\n",
    "dataset_type = 'blobs'  # try: blobs, classification, moons, circles, anisotropic, varied\n",
    "\n",
    "if dataset_type == 'blobs':\n",
    "    X1, _ = make_blobs(\n",
    "        n_samples=500,\n",
    "        centers=3,\n",
    "        cluster_std=3,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "elif dataset_type == 'classification':\n",
    "    X1, _ = make_classification(\n",
    "        n_samples=120,\n",
    "        n_features=2,\n",
    "        n_redundant=0,\n",
    "        n_clusters_per_class=1,\n",
    "        n_classes=3,\n",
    "        class_sep=2,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "elif dataset_type == 'moons':\n",
    "    from sklearn.datasets import make_moons\n",
    "    X1, _ = make_moons(\n",
    "        n_samples=120,\n",
    "        noise=0.1,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "elif dataset_type == 'circles':\n",
    "    from sklearn.datasets import make_circles\n",
    "    X1, _ = make_circles(\n",
    "        n_samples=500,\n",
    "        noise=0.05,\n",
    "        factor=0.5,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "elif dataset_type == 'anisotropic':\n",
    "    X1, _ = make_blobs(\n",
    "        n_samples=120,\n",
    "        centers=3,\n",
    "        cluster_std=1,\n",
    "        random_state=42\n",
    "    )\n",
    "    # Apply an anisotropic linear transformation\n",
    "    transformation = [[0.6, -0.6], [-0.4, 0.8]]\n",
    "    X1 = X1.dot(transformation)\n",
    "\n",
    "elif dataset_type == 'varied':\n",
    "    X1, _ = make_blobs(\n",
    "        n_samples=120,\n",
    "        centers=[[-5, -5], [0, 0], [5, 5]],\n",
    "        cluster_std=[1.0, 2.5, 0.5],\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "else:\n",
    "    raise ValueError(\"Unknown dataset_type\")\n",
    "\n",
    "# Normalize and add optional noise\n",
    "n_noise = 0\n",
    "X1 -= np.mean(X1, axis=0)\n",
    "MAX = np.max(np.abs(X1))\n",
    "X2 = np.random.uniform(-MAX, MAX, size=(n_noise, 2))\n",
    "\n",
    "X = np.concatenate((X1, X2), axis=0)\n",
    "X -= np.mean(X, axis=0)\n",
    "X /= np.std(X)\n",
    "MAX = np.max(np.abs(X)) * 1.2\n",
    "\n",
    "# Plot the data\n",
    "plt.figure(figsize=(3, 3))\n",
    "plt.scatter(X[:, 0], X[:, 1], c='b', s=3)\n",
    "plt.xlim(-MAX, MAX)\n",
    "plt.ylim(-MAX, MAX)\n",
    "plt.gca().set_aspect('equal')\n",
    "plt.title(f\"Dataset: {dataset_type}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f2e6025",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just to visualize the density, we create a histogram\n",
    "\n",
    "# Calculate 2D histogram\n",
    "hist, xedges, yedges = np.histogram2d(X[:,0], X[:,1], bins=20)\n",
    "\n",
    "# Construct arrays for the X-Y edges\n",
    "xpos, ypos = np.meshgrid(xedges[:-1], yedges[:-1], indexing=\"ij\")\n",
    "\n",
    "# Create 3D plot\n",
    "fig = plt.figure(figsize=(6, 6))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "# Plot 3D surface\n",
    "surf = ax.plot_surface(xpos, ypos, hist, cmap='viridis')\n",
    "\n",
    "# Set labels and title\n",
    "ax.set_xlabel('X')\n",
    "ax.set_ylabel('Y')\n",
    "ax.set_xlim(-MAX, MAX)\n",
    "ax.set_ylim(-MAX, MAX)\n",
    "ax.set_zlabel('Frequency')\n",
    "ax.set_title('2D Histogram')\n",
    "\n",
    "# Add color bar\n",
    "fig.colorbar(surf)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab1af422",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "07171622",
   "metadata": {},
   "source": [
    "### MeanShift Clustering in-depth\n",
    "\n",
    "##### How Mean Shift Clustering Works (Mathematical Explanation)\n",
    "\n",
    "Mean Shift is a **mode-seeking algorithm** that iteratively shifts *kernel centers* toward the nearest high-density region.  \n",
    "**The data points do not move** — they define the density landscape.\n",
    "\n",
    "---\n",
    "\n",
    "**1. Kernel Density Estimate (KDE)**\n",
    "\n",
    "At its core, Mean Shift uses a kernel to estimate the local probability density function (PDF) around each point.\n",
    "\n",
    "The multivariate KDE at point $\\mathbf{x}$ is:\n",
    "\n",
    "$$\n",
    "\\hat{f}(\\mathbf{x}) = \\frac{1}{n h^d} \\sum_{i=1}^n K\\!\\Big(\\frac{\\mathbf{x} - \\mathbf{x}_i}{h}\\Big)\n",
    "$$\n",
    "\n",
    "- $n$ = number of data points  \n",
    "- $h$ = bandwidth (kernel size)  \n",
    "- $d$ = number of dimensions  \n",
    "- $K(\\cdot)$ = kernel function (e.g., Gaussian, flat)\n",
    "\n",
    "---\n",
    "\n",
    "**2. The Mean Shift Vector**\n",
    "\n",
    "The **mean shift vector** is the gradient of the density estimate — it points toward the direction of **maximum increase in density**.\n",
    "\n",
    "At a point $\\mathbf{x}$:\n",
    "\n",
    "$$\n",
    "\\mathbf{m}(\\mathbf{x}) = \\frac{ \\sum_{i=1}^n \\mathbf{x}_i \\, K\\!\\Big(\\frac{\\mathbf{x} - \\mathbf{x}_i}{h}\\Big) }{ \\sum_{i=1}^n K\\!\\Big(\\frac{\\mathbf{x} - \\mathbf{x}_i}{h}\\Big) } - \\mathbf{x}\n",
    "$$\n",
    "\n",
    "So, it’s the difference between the **weighted mean of nearby points** and the current position.\n",
    "\n",
    "---\n",
    "\n",
    "**3. Iterative Update**\n",
    "\n",
    "Given a starting point $\\mathbf{x}^{(t)}$ (the kernel center):\n",
    "\n",
    "$$\n",
    "\\mathbf{x}^{(t+1)} = \\mathbf{x}^{(t)} + \\mathbf{m}(\\mathbf{x}^{(t)})\n",
    "$$\n",
    "\n",
    "Equivalently:\n",
    "\n",
    "$$\n",
    "\\mathbf{x}^{(t+1)} = \\frac{ \\sum_{i=1}^n \\mathbf{x}_i \\, K\\!\\Big(\\frac{\\mathbf{x}^{(t)} - \\mathbf{x}_i}{h}\\Big) }{ \\sum_{i=1}^n K\\!\\Big(\\frac{\\mathbf{x}^{(t)} - \\mathbf{x}_i}{h}\\Big) }\n",
    "$$\n",
    "\n",
    "This means the kernel center shifts to the **weighted mean** of the data points inside its window.\n",
    "\n",
    "---\n",
    "\n",
    "**4. Convergence**\n",
    "\n",
    "The mean shift process repeats until the shift vector $\\mathbf{m}(\\mathbf{x})$ is very small:\n",
    "\n",
    "$$\n",
    "\\|\\mathbf{m}(\\mathbf{x})\\| < \\epsilon\n",
    "$$\n",
    "\n",
    "for some small threshold $\\epsilon$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bb8a83a",
   "metadata": {},
   "source": [
    "### Using Mean Shift Clustering in scikit-learn\n",
    "\n",
    "**Important Parameters**\n",
    "\n",
    "**bandwidth**  \n",
    "The `bandwidth` parameter defines the radius of the kernel window used for shifting. A small bandwidth finds fine-grained clusters, while a large bandwidth merges clusters into larger groups. If you do not specify a bandwidth, scikit-learn can estimate a good starting value automatically using a built-in heuristic.\n",
    "\n",
    "**bin_seeding**  \n",
    "This parameter, when set to true, uses an initial coarse grid of seed points rather than starting with every data point. This speeds up convergence, especially for large datasets.\n",
    "\n",
    "**cluster_all**  \n",
    "This parameter determines whether all data points must be assigned to a cluster. If true (the default), every point is clustered. If false, points that do not fall within any kernel window are marked as noise and labeled as -1.\n",
    "\n",
    "**What You Get After Fitting**\n",
    "\n",
    "After running Mean Shift, the fitted model provides:\n",
    "\n",
    "- Cluster labels for each sample in the data.\n",
    "- The coordinates of the cluster centers that represent the detected modes.\n",
    "- The total number of clusters found by the algorithm.\n",
    "\n",
    "**Practical Tips**\n",
    "\n",
    "- The default kernel in scikit-learn’s Mean Shift is a flat kernel.\n",
    "- Mean Shift can be computationally expensive for large datasets, so using `bin_seeding` or subsampling can help.\n",
    "- Selecting a good bandwidth is crucial for meaningful clustering results. You can experiment with different values or use scikit-learn’s automatic bandwidth estimation as a starting point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eab8e68a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import MeanShift\n",
    "\n",
    "# Important Parameters\n",
    "bandwidth = 0.4\n",
    "model = MeanShift(bandwidth=bandwidth)\n",
    "\n",
    "# Fit the model and predict cluster labels\n",
    "model.fit(X)\n",
    "labels = model.predict(X)\n",
    "\n",
    "# print the results\n",
    "print(\"Number of clusters found:\", len(np.unique(labels)))\n",
    "print(labels)\n",
    "\n",
    "# print cluster centers\n",
    "print(\"Cluster centers:\\n\", model.cluster_centers_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90f1bb99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the clustered data\n",
    "plt.figure(figsize=(3, 3))\n",
    "plt.scatter(X[:, 0], X[:, 1], c=labels, s=5)\n",
    "plt.scatter(model.cluster_centers_[:, 0], model.cluster_centers_[:, 1], c='red', s=50, marker='X')  # Mark cluster centers\n",
    "plt.xlim(-MAX, MAX)\n",
    "plt.ylim(-MAX, MAX)\n",
    "plt.gca().set_aspect('equal')\n",
    "plt.title(f\"Mean Shift Clustering (bandwidth={bandwidth})\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f0a5cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's try to estimate the bandwidth using the built-in function\n",
    "from sklearn.cluster import estimate_bandwidth\n",
    "bandwidth = estimate_bandwidth(X, quantile=0.2)\n",
    "print(\"Estimated bandwidth:\", bandwidth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c503ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# recalculate with new bandwidth\n",
    "modelNew = MeanShift(bandwidth=bandwidth)\n",
    "modelNew.fit(X)\n",
    "labelsNew = modelNew.predict(X)\n",
    "print(\"Number of clusters found with estimated bandwidth:\", len(np.unique(labelsNew)))\n",
    "\n",
    "# Plot the clustered data\n",
    "plt.figure(figsize=(3, 3))\n",
    "plt.scatter(X[:, 0], X[:, 1], c=labelsNew, s = 5)\n",
    "plt.scatter(modelNew.cluster_centers_[:, 0], modelNew.cluster_centers_[:, 1], c='red', s=50, marker='X')  # Mark cluster centers\n",
    "plt.xlim(-MAX, MAX)\n",
    "plt.ylim(-MAX, MAX)\n",
    "plt.gca().set_aspect('equal')\n",
    "plt.title(f\"Mean Shift Clustering (bandwidth={bandwidth})\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8efb506a",
   "metadata": {},
   "source": [
    "### Cluster Stability & Agreement Demo\n",
    "\n",
    "In real-world clustering, we often want to check **how stable** or **consistent** our clusters are across different methods or parameter settings.\n",
    "\n",
    "One simple approach is to run multiple clustering algorithms (or the same algorithm with different `k` or distance thresholds) and compare how often pairs of points get assigned to the **same cluster**.\n",
    "\n",
    "In this demo, we:\n",
    "- Run several clustering variants (K-Means with different `k`, GMM).\n",
    "- For each pair of points, count how many times they co-occur in the same cluster.\n",
    "- Visualize the **agreement matrix** as a heatmap to see which regions of the dataset are robustly grouped together.\n",
    "\n",
    "This teaches how to **test cluster robustness** and **explore consensus** across methods.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "040522d6",
   "metadata": {},
   "source": [
    "When you run two different clustering models (or the same model with different settings), it’s helpful to check **how their cluster assignments agree**.\n",
    "\n",
    "A **cross-tabulation** (contingency table) counts how many points from each cluster in `model1` overlap with each cluster in `model2`.  \n",
    "This shows:\n",
    "- Which clusters align well across runs.\n",
    "- Whether clusters split or merge when parameters change.\n",
    "- How stable or unstable your clusters are.\n",
    "\n",
    "It’s a simple but powerful way to **diagnose clustering consistency** — especially when you don’t have ground truth labels!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f2cbf43",
   "metadata": {},
   "source": [
    "Let's create a plot function to ease us so we don't have to repeat the same lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a38a2a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_regions(model):\n",
    "    \n",
    "    nGrid = 200\n",
    "    a1 = np.linspace(-MAX,MAX,nGrid)\n",
    "    a2 = np.linspace(-MAX,MAX,nGrid)\n",
    "    \n",
    "    A1, A2 = np.meshgrid(a1,a2)\n",
    "    A1 = A1.flatten()\n",
    "    A2 = A2.flatten()\n",
    "    A = np.vstack((A1,A2)).T\n",
    "    \n",
    "    B = model.predict(A)\n",
    "    B = B.reshape(nGrid,nGrid)\n",
    "    B = np.flipud(B)\n",
    "    \n",
    "    return B"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7793e530",
   "metadata": {},
   "source": [
    "**Run method 1: kmeans**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "829b691f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "model1 = KMeans(n_clusters=3)\n",
    "model1.fit(X)\n",
    "labels1 = model1.predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3252194",
   "metadata": {},
   "source": [
    "**Run method 2: GMM**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5737e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.mixture import GaussianMixture as GMM\n",
    "\n",
    "model2 = GMM(n_components=3)\n",
    "model2.fit(X)\n",
    "labels2 = model2.predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7196ff50",
   "metadata": {},
   "source": [
    "**Create cross-tab of cluster assignments**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "607dae8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\n",
    "    'model1': labels1,\n",
    "    'model2': labels2,\n",
    "})\n",
    "ctab = pd.crosstab(df['model1'], df['model2'])\n",
    "\n",
    "# just display the cross-table\n",
    "display(ctab)\n",
    "# or plot a heatmap - which looks cooler\n",
    "plt.figure(figsize=(5, 4))\n",
    "sns.heatmap(ctab, annot=True, square=True, fmt='d')\n",
    "plt.title(\"Cluster Cross-Tabulation (model1 vs model2)\")\n",
    "plt.xlabel(\"model2 Clusters\")\n",
    "plt.ylabel(\"model1 Clusters\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e595ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the data for both models side by side to compare \n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=labels1, s=5)\n",
    "plt.scatter(model1.cluster_centers_[:, 0], model1.cluster_centers_[:, 1], c='red', s=50, marker='X')  # Mark cluster centers\n",
    "plt.xlim(-MAX, MAX)\n",
    "plt.ylim(-MAX, MAX)\n",
    "plt.gca().set_aspect('equal')\n",
    "plt.title(f\"KMeans Clustering (k=3)\")\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=labels2, s=5)\n",
    "plt.xlim(-MAX, MAX)\n",
    "plt.ylim(-MAX, MAX)\n",
    "plt.gca().set_aspect('equal')\n",
    "plt.title(f\"GMM Clustering (k=3)\")\n",
    "plt.show()\n",
    "\n",
    "# Consensus Clustering\n",
    "# ----------------------------------------\n",
    "# One challenge with clustering is that the results can be sensitive to the choice of algorithm and its parameters.\n",
    "# ----------------------------------------\n",
    "# A robust clustering should yield similar groupings even when the method or parameters change slightly.\n",
    "# ----------------------------------------\n",
    "# A simple approach is to run multiple clustering algorithms (or the same algorithm with different `k` or distance thresholds) and compare how often pairs of points get assigned to the **same cluster**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b264a21",
   "metadata": {},
   "source": [
    "Note: There is a possibility that the cluster numbers are switched, depending on the methods and which points are assigned first. So you might want to switch the numbers and compare again.\n",
    "\n",
    "As seen on the plot above, the green and purple clusters are switched."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a596ba75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you want, we can switch the labels zero to 1 in model 2\n",
    "labels2_switched = np.where(labels2 == 0, 1, np.where(labels2 == 1, 0, labels2))\n",
    "\n",
    "# recalculate the cross-tab with switched labels\n",
    "df_switched = pd.DataFrame({\n",
    "    'model1': labels1,\n",
    "    'model2': labels2_switched,\n",
    "})\n",
    "ctab_switched = pd.crosstab(df_switched['model1'], df_switched['model2'])\n",
    "# just display the cross-table\n",
    "display(ctab_switched)\n",
    "# or plot a heatmap - which looks cooler\n",
    "plt.figure(figsize=(5, 4))\n",
    "sns.heatmap(ctab_switched, annot=True, square=True, fmt='d')\n",
    "plt.title(\"Cluster Cross-Tabulation (model1 vs model2) - switched\")\n",
    "plt.xlabel(\"model2 Clusters\")\n",
    "plt.ylabel(\"model1 Clusters\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0061b291",
   "metadata": {},
   "source": [
    "### Resampling-Based Cluster Stability\n",
    "\n",
    "One practical way to test **cluster stability** is to run the same clustering algorithm many times on slightly different versions of your dataset.\n",
    "\n",
    "**Two common ways:**\n",
    "- **Subsampling:** Randomly pick a fraction (e.g., 80%) of the data each time.\n",
    "- **Bootstrapping:** Sample *with replacement* so the new sample is the same size as the original (some points repeated, some missing).\n",
    "\n",
    "For each run:\n",
    "- Fit K-Means (or other method).\n",
    "- Record which pairs of points end up in the same cluster.\n",
    "- Average over runs → you get a **co-occurrence matrix** that shows how consistently pairs of points are grouped together.\n",
    "- Use matrix for precomputed spectral clustering\n",
    "\n",
    "Interpreting results:\n",
    "- Co-occurrence values close to 1 means the points are usually clustered together\n",
    "- Values close to 0 means the points are usually NOT clustered together\n",
    "- Values near 0.5 are uncertain and can go either way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb514ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------\n",
    "# Build co-occurrence matrix + store all labels\n",
    "# --------------------------------\n",
    "N = X.shape[0]\n",
    "\n",
    "# Parameters\n",
    "n_clusters = 3\n",
    "n_runs = 100\n",
    "method = 'subsample'\n",
    "subsample_frac = 0.8\n",
    "\n",
    "co_matrix = np.zeros((N, N)) # initialize co-occurrence matrix\n",
    "labels_runs = [] # to store all labels from each run\n",
    "rng = np.random.default_rng(42) # for reproducibility\n",
    "\n",
    "for run in range(n_runs):\n",
    "\n",
    "    # Create resampled dataset\n",
    "    if method == 'subsample':\n",
    "        indices = rng.choice(N, size=int(N * subsample_frac), replace=False)\n",
    "    elif method == 'bootstrap':\n",
    "        indices = rng.choice(N, size=N, replace=True)\n",
    "    else:\n",
    "        raise ValueError(\"method must be 'subsample' or 'bootstrap'\")\n",
    "\n",
    "    # new variable to hold the resampled data based on indices\n",
    "    X_resample = X[indices]\n",
    "\n",
    "    # Fit clustering model on resampled data\n",
    "    model = KMeans(n_clusters=n_clusters, random_state=0)\n",
    "    model.fit(X_resample)\n",
    "\n",
    "    # Predict for all points\n",
    "    labels_full = model.predict(X)\n",
    "    labels_runs.append(labels_full)\n",
    "\n",
    "    # Update co-occurrence for all pairs once (upper triangle)\n",
    "    for i in range(N):\n",
    "        for j in range(i + 1, N):\n",
    "            if labels_full[i] == labels_full[j]:\n",
    "                co_matrix[i, j] += 1\n",
    "                co_matrix[j, i] += 1  # mirror\n",
    "\n",
    "    # Also count self-coincidence\n",
    "    co_matrix[np.arange(N), np.arange(N)] += 1\n",
    "\n",
    "co_matrix /= n_runs # normalize to [0, 1]\n",
    "\n",
    "# Show heatmap\n",
    "plt.figure(figsize=(7, 6))\n",
    "sns.heatmap(co_matrix, square=True, vmin=0, vmax=1, cmap='nipy_spectral')\n",
    "plt.title(f\"Cluster Co-Occurrence Matrix\\n(method = {method}, runs = {n_runs})\")\n",
    "plt.xlabel(\"Point Index\")\n",
    "plt.ylabel(\"Point Index\")\n",
    "plt.show()\n",
    "\n",
    "# In this heatmap, most of them are black and white. This means that most of the points are either always clustered together (white) or never clustered together (black) across the multiple runs. This indicates a high level of stability in the clustering results, as the same pairs of points consistently end up in the same clusters regardless of the specific run or subsampling.\n",
    "\n",
    "# If we have different colors (not just black and white), it indicates that some pairs of points are inconsistently clustered together across different runs. This suggests that the clustering results are less stable, as the same pairs of points do not consistently end up in the same clusters. The presence of various shades of gray or colors in the heatmap reflects this variability in clustering assignments, indicating that the clustering method may be sensitive to the choice of parameters or the specific subset of data used in each run."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
