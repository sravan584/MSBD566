{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c9c27305",
   "metadata": {},
   "source": [
    "# MSBD566 - Lecture 6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "521b7153",
   "metadata": {},
   "source": [
    "## Classification [continued]\n",
    "\n",
    "Most of the example and notes are taken from here: \n",
    "https://github.com/Jpickard1/Data-Analytics-for-Biomedical-Research/blob/main/Tutorial.ipynb\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "616aaf0c",
   "metadata": {},
   "source": [
    "### Loading data and Python Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6111f3e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.inspection import DecisionBoundaryDisplay\n",
    "from sklearn.metrics import ConfusionMatrixDisplay, accuracy_score, classification_report, roc_curve, RocCurveDisplay\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f7c591",
   "metadata": {},
   "source": [
    "For datasets, we will use the UCI Machine Learning Repository via the ucimlrepo package. We will need to install it first.\n",
    "Depending on your environment, you can use either of the following commands to install the package.\n",
    "\n",
    "`%pip install ucimlrepo`\n",
    "\n",
    "or \n",
    "\n",
    "`!pip install ucimlrepo`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "452de287",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import datasets module\n",
    "%pip install ucimlrepo # or use \"!pip install ucimlrepo\" if you are not using iPython Magic Command.\n",
    "from ucimlrepo import fetch_ucirepo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43004e3b",
   "metadata": {},
   "source": [
    "**Data Information**\n",
    "\n",
    "Problems of real-life complexity are needed to test and compare various data mining and pattern recognition methods. The proposed database can be used to solve two practically important problems: predicting complications of Myocardial Infarction (MI) based on information about the patient (i) at the time of admission and (ii) on the third day of the hospital period. Another important group of tasks is phenotyping of disease (cluster analysis), dynamic phenotyping (filament extraction and identification of disease trajectories) and visualisation (disease mapping). \n",
    "MI is one of the most challenging problems of modern medicine. Acute myocardial infarction is associated with high mortality in the first year after it. The incidence of MI remains high in all countries. This is especially true for the urban population of highly developed countries, which is exposed to chronic stress factors, irregular and not always balanced nutrition. In the United States, for example, more than a million people suffer from MI every year, and 200-300 thousand of them die from acute MI before arriving at the hospital. \n",
    "The course of the disease in patients with MI is different. MI can occur without complications or with complications that do not worsen the long-term prognosis. At the same time, about half of patients in the acute and subacute periods have complications that lead to worsening of the disease and even death. Even an experienced specialist can not always foresee the development of these complications. In this regard, predicting complications of myocardial infarction in order to timely carry out the necessary preventive measures is an important task. \n",
    "\n",
    "**Problems to solve**\n",
    "\n",
    "In general columns 2-112 can be used as input data for prediction. Possible complications (outputs) are listed in columns 113-124.\n",
    "There are four possible time moments for complication prediction: on base of the information known at\n",
    "1.\tthe time of admission to hospital: all input columns (2-112) except 93, 94, 95, 100, 101, 102, 103, 104, 105 can be used for prediction;\n",
    "2.\tthe end of the first day (24 hours after admission to the hospital): all input columns (2-112) except 94, 95, 101, 102, 104, 105 can be used for prediction;\n",
    "3.\tthe end of the second day (48 hours after admission to the hospital) all input columns (2-112) except 95, 102, 105 can be used for prediction;\n",
    "4.\tthe end of the third day (72 hours after admission to the hospital) all input columns (2-112) can be used for prediction.\n",
    "\n",
    "You can find detailed description of database, descriptive statistics and csv version of database in DOI: 10.25392/leicester.data.12045261.v3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e13e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dataset\n",
    "# Source: https://archive.ics.uci.edu/dataset/579/myocardial+infarction+complications\n",
    "complications = fetch_ucirepo(id=579)\n",
    "complications.data.features\n",
    "\n",
    "# Split features vs predictors\n",
    "X = complications.data.features # Get features\n",
    "y = complications.data.targets  # Get predictors\n",
    "\n",
    "# Fill missing values with 0\n",
    "X = X.fillna(0)\n",
    "y = y.fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5c58509",
   "metadata": {},
   "source": [
    "We are going to focus on Fiber Prediction (FIBR_PREDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0350158",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Choose a target to predict\n",
    "y = y['FIBR_PREDS']\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                    test_size=0.4, # 40% of the data will be used for testing\n",
    "                                                    random_state=42 # Set a random state for reproducibility\n",
    "                                                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7a7a498",
   "metadata": {},
   "source": [
    "### K-nearest neighbor (KNN)\n",
    "\n",
    "Not to be confused with the neural network which has 'NN' term as well, this is a simple statistical that is not related to the deep learning.\n",
    "\n",
    "`sklearn`'s KNN algorithm is implemented in the `KNeighbordsClassifier`. 2 inputs are needed\n",
    "* `n_neighbors`: is the number of neighbors that each unknown datapoint must be compared to\n",
    "* `metric`: is the distance measure that is used to compare the similarity of different datapoints. Options: 'euclidian', 'manhattan', 'minkowski', 'chebyshev'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4e27144",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_model = KNeighborsClassifier(n_neighbors=1, metric='euclidean') #other metrics (distance measure): 'manhattan', 'minkowski', 'chebyshev'\n",
    "knn_model.fit(X_train, y_train)\n",
    "knn_predictions = knn_model.predict(X_test)\n",
    "knn_accuracy = accuracy_score(y_test, knn_predictions)\n",
    "print(\"K-Nearest Neighbors Accuracy:\", knn_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a3340f1",
   "metadata": {},
   "source": [
    "### Naive Bayes Model\n",
    "\n",
    "`sklearn`'s Naive Bayes Model is implemented in the `GaussianNB`. Unlike KNN, this algorithm is parametric, meaning that the parameters for a probability distribution are learned during the training. No input is needed other than the training set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91fdf82c",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_model = GaussianNB()\n",
    "nb_model.fit(X_train, y_train)\n",
    "nb_predictions = nb_model.predict(X_test)\n",
    "nb_accuracy = accuracy_score(y_test, nb_predictions)\n",
    "print(\"Naive Bayes Accuracy:\", nb_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0a71b2e",
   "metadata": {},
   "source": [
    "### Decision Tree\n",
    "\n",
    "`sklearn`'s Decision Tree algorithm is implemented in the `DecisionTreeClassifier`. It uses entropy as the criterion to measure the quality of splits and employs a random strategy for choosing the best split at each node.  The tree is limited to a maximum depth of 5 levels, and leaf nodes must contain at least 10 samples.\n",
    "\n",
    "* `criterion`: The function to measure the quality of a split. In this case, `'entropy'` is used, which calculates the information gain based on the entropy.\n",
    "* `splitter`: The strategy used to choose the split at each node. `'random'` means that random splits are considered, as opposed to `'best'` which would choose the best split.\n",
    "* `max_depth`: maximum depth of the tree. Tree growth is halted once this depth is reached.\n",
    "* `min_samples_leaf`: Specifies the minimum number of samples required to be at a leaf node. If a split results in a leaf node with fewer samples than this, the split is not considered valid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c978451",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_model = DecisionTreeClassifier(criterion='entropy', splitter='random', max_depth=5, min_samples_leaf=10)\n",
    "dt_model.fit(X_train, y_train)\n",
    "dt_predictions = dt_model.predict(X_test)\n",
    "dt_accuracy = accuracy_score(y_test, dt_predictions)\n",
    "print(\"Decision Tree Accuracy:\", dt_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b10f0f7",
   "metadata": {},
   "source": [
    "### Random Forest\n",
    "\n",
    "`sklearn`'s random Forest algorithm is implemented in the `RandomForestClassifier`. It uses entropy as the criterion to measure the quality of splits, allows each tree to grow to a maximum depth of 10 levels, and requires at least 1 sample in each leaf node.\n",
    "\n",
    "* `n_estimators`: Number of trees in the forest\n",
    "* `criterion`: The function to measure the quality of a split. In this case, `'entropy'` is used, which calculates the information gain based on the entropy.\n",
    "* `max_depth`: maximum depth of the tree. Tree growth is halted once this depth is reached.\n",
    "* `min_samples_leaf`: Specifies the minimum number of samples required to be at a leaf node. If a split results in a leaf node with fewer samples than this, the split is not considered valid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea5466c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_model = RandomForestClassifier(n_estimators=100, criterion='entropy', max_depth=10, min_samples_leaf=1)\n",
    "rf_model.fit(X_train, y_train)\n",
    "rf_predictions = rf_model.predict(X_test)\n",
    "rf_accuracy = accuracy_score(y_test, rf_predictions)\n",
    "print(\"Random Forest Accuracy:\", rf_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99e383b2",
   "metadata": {},
   "source": [
    "### Support Vector Machine (SVM)\n",
    "\n",
    "`sklearn`'s SVM algorithm is implemented in the `SVC`. It performs classification by finding the hyperplane that best separates two classes in an N-dimensional space (where N is the number of features).\n",
    "\n",
    "Here we set the number of iterations to 1000. In this example, we represent our data with 111 features (column 2-112) in 111 dimensional space using standard distance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad8c4d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_model = SVC(max_iter=1000)\n",
    "svm_model.fit(X_train, y_train)\n",
    "svm_predictions = svm_model.predict(X_test)\n",
    "svm_accuracy = accuracy_score(y_test, svm_predictions)\n",
    "print(\"SVM Accuracy:\", svm_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e70c5318",
   "metadata": {},
   "source": [
    "### Kernel Learning\n",
    "\n",
    "`sklearn`'s SVM algorithm is implemented in the `SVC` with several kernels options. In regular SVM, the kernel=`rbf` is used as default. It performs classification by finding the hyperplane that best separates two classes in an N-dimensional space (where N is the number of features). Different kernels will give different distance function in the `SVC` function. \n",
    "\n",
    "Same as above, here, we set the number of iterations to 1000. In this example, we represent our data with 111 features (column 2-112) in 111 dimensional space. Options that we can set for the distance kernels are possibly `poly`, `rbf`, `sigmoid`, `precomputed`. We will see the performace without a kernel function by using `linear` option."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a0104f",
   "metadata": {},
   "outputs": [],
   "source": [
    "svmLinear_model = SVC(kernel='linear', max_iter=1000)\n",
    "svmLinear_model.fit(X_train, y_train)\n",
    "svmLinear_predictions = svmLinear_model.predict(X_test)\n",
    "svmLinear_accuracy = accuracy_score(y_test, svmLinear_predictions)\n",
    "print(\"SVM (no kernel) Accuracy:\", svmLinear_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d29c2e7a",
   "metadata": {},
   "source": [
    "## Model Comparison & Cross Validation\n",
    "\n",
    "We will use several metrics to do compare the model performance:\n",
    "1. accuracy score\n",
    "2. precision score\n",
    "3. recall score\n",
    "4. f1 score\n",
    "\n",
    "Several cross-validation methods:\n",
    "1. K-fold\n",
    "2. Stratified K-fold\n",
    "3. Leave-one-out\n",
    "4. Leave-p-out\n",
    "\n",
    "For classification, we will use stratified K-fold method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89465b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, make_scorer\n",
    "from sklearn.model_selection import cross_validate, StratifiedKFold, cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfed75db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define cross-validation technique\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Models and parameters\n",
    "knn = KNeighborsClassifier()\n",
    "dTree = DecisionTreeClassifier(random_state=42)\n",
    "RF = RandomForestClassifier(random_state=42)\n",
    "NB = GaussianNB()\n",
    "SVM = SVC(random_state=42)\n",
    "SVM_linear = SVC(kernel='linear', random_state=42)\n",
    "\n",
    "# set models in a dictionary\n",
    "models = [\n",
    "    knn,\n",
    "    dTree,\n",
    "    RF\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4987c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scoring model\n",
    "score = 'accuracy' # other options: 'precision', 'recall', 'f1'\n",
    "knn_cv_results = cross_validate(knn, X, y, cv=cv, scoring=score)\n",
    "dTree_cv_results = cross_validate(dTree, X, y, cv=cv, scoring=score)\n",
    "RF_cv_results = cross_validate(RF, X, y, cv=cv, scoring=score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dec5ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the score results\n",
    "knn_scores = knn_cv_results['test_score']\n",
    "dTree_scores = dTree_cv_results['test_score']\n",
    "RF_scores = RF_cv_results['test_score']\n",
    "\n",
    "# visualize the results\n",
    "models = ['KNN', 'Decision Tree', 'Random Forest']\n",
    "scores = [knn_scores, dTree_scores, RF_scores]\n",
    "plt.boxplot(scores, tick_labels=models)\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Model Comparison using Cross-Validation')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46ff080c",
   "metadata": {},
   "source": [
    "You can visualize the statistics of the training results using `cross_val_score()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4675dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for m in models:\n",
    "    print(m)\n",
    "    cv_scores = cross_val_score(m, X, y, cv=cv, scoring='f1')\n",
    "\n",
    "    display(pd.DataFrame(cv_scores).describe().T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bc6f028",
   "metadata": {},
   "source": [
    "To compare across different metrics at once, we can set in the cross_validation function to get all other scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6179103b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare different metrics at once\n",
    "metrics = {\n",
    "    'accuracy': make_scorer(accuracy_score),\n",
    "    'precision': make_scorer(precision_score, average='weighted', zero_division=0),\n",
    "    'recall': make_scorer(recall_score, average='weighted', zero_division=0),\n",
    "    'f1': make_scorer(f1_score, average='weighted', zero_division=0)\n",
    "}\n",
    "knn_cv_all_results = cross_validate(knn, X, y, cv=cv, scoring=metrics)\n",
    "dTree_cv_all_results = cross_validate(dTree, X, y, cv=cv, scoring=metrics)\n",
    "RF_cv_all_results = cross_validate(RF, X, y, cv=cv, scoring=metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dddfddaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize all metrics in knn\n",
    "KNN_metrics = pd.DataFrame(knn_cv_all_results).filter(like='test_')\n",
    "KNN_metrics.columns = ['Accuracy', 'Precision', 'Recall', 'F1-score']\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(data=KNN_metrics)\n",
    "plt.title('KNN Model Performance Metrics')\n",
    "plt.ylabel('Score')\n",
    "plt.ylim(0, 1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcb4aa30",
   "metadata": {},
   "source": [
    "## Activity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8493e8aa",
   "metadata": {},
   "source": [
    "### Team Members\n",
    "Write your name here:\n",
    "[YOUR NAME]\n",
    "\n",
    "Write your group member's name here: \n",
    "1. name 1\n",
    "2. name 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08fbfa02",
   "metadata": {},
   "source": [
    "### Questions\n",
    "1. Compare the regular splitting of training and test (random splitting) method vs Stratified Kfold method when to run the Random Forest classification. Which one has better metrics? Use accuracy and precision metrics in a side-by-side bar plots.\n",
    "\n",
    "2. In kernel learning classification, compare 'rbf', 'precomputed', and 'linear' distance calculations. Use a type of plot to visualize. Which has better accuracy? Which has better precision? \n",
    "\n",
    "3. Plot 2 boxplots for metrics of running 4 classification methods of your choice. Use stratified K-fold cross validation for this training: \n",
    "    (a) F1-score plot\n",
    "    (b) Precision score plot\n",
    "\n",
    "4. Explore different target variables in the dataset and choose the same classification methods to run. Use the stratified K-fold to split the data into training and test sets. Does the same method show the best precision score?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e32493fb",
   "metadata": {},
   "source": [
    "### Answers"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
